{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b8eac40dc40dfee193fc75e448d7129280d537d209f6594539fc58c0f1d96d80",
   "display_name": "Python 3.8.8 64-bit ('chem': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n(2048,)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "atorvastatin_smiles = 'O=C(O)C[C@H](O)C[C@H](O)CCn2c(c(c(c2c1ccc(F)cc1)c3ccccc3)C(=O)Nc4ccccc4)C(C)C'\n",
    "atorvastatin = Chem.MolFromSmiles(atorvastatin_smiles) # Atorvastatin (aka Lipitor) is one of the world's best-selling drugs.\n",
    "\n",
    "fingerprint = GetMorganFingerprintAsBitVect(atorvastatin, radius=2, nBits=2048)\n",
    "\n",
    "fp_array = np.zeros((1, ))\n",
    "ConvertToNumpyArray(fingerprint, fp_array)\n",
    "\n",
    "# Fingerprints\n",
    "print(fp_array)\n",
    "# [0. 1. 0. ... 0. 0. 0.]\n",
    "\n",
    "print(fp_array.shape)\n",
    "# (2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "source": [
    "![alt text](../metadata/Snipaste_2021-05-21_20-26-00.png \"explanation\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2a. Atom Features and bond connections (edge indices)\n",
    "\n",
    "We will use these atom features:\n",
    "\n",
    "a) Atomic number (which determines atom type as well)\n",
    "\n",
    "b) The number of hydrogens attached to the atom.\n",
    "\n",
    "These are basic features but sufficient for our purposes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return [x == s for s in allowable_set]\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return [x == s for s in allowable_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(mol):\n",
    "    atomic_number = []\n",
    "    num_hs = []\n",
    "    \n",
    "    # for atom in mol.GetAtoms():\n",
    "    #     # print(atom.GetAtomicNum())\n",
    "    #     # 原子编号\n",
    "    #     atomic_number.append(atom.GetAtomicNum())\n",
    "    #     num_hs.append(atom.GetTotalNumHs(includeNeighbors=True))\n",
    "    \n",
    "    AttentiveFP = []\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        AttentiveFP += [\n",
    "            one_of_k_encoding_unk(\n",
    "            atom.GetSymbol(),\n",
    "            [\n",
    "                'B',\n",
    "                'C',\n",
    "                'N',\n",
    "                'O',\n",
    "                'F',\n",
    "                'Si',\n",
    "                'P',\n",
    "                'S',\n",
    "                'Cl',\n",
    "                'As',\n",
    "                'Se',\n",
    "                'Br',\n",
    "                'Te',\n",
    "                'I',\n",
    "                'At',\n",
    "                'other'\n",
    "            ]) + one_of_k_encoding(atom.GetDegree(),\n",
    "                                    [0, 1, 2, 3, 4, 5]) + \\\n",
    "                    [atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \\\n",
    "                    one_of_k_encoding_unk(atom.GetHybridization(), [\n",
    "                        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "                        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.\n",
    "                                            SP3D, Chem.rdchem.HybridizationType.SP3D2,'other'\n",
    "                    ]) + [atom.GetIsAromatic()]\n",
    "                    + one_of_k_encoding_unk(atom.GetTotalNumHs(),\n",
    "                                                      [0, 1, 2, 3, 4])\n",
    "        ]\n",
    "\n",
    "    # return torch.tensor([atomic_number, num_hs], dtype=torch.float).t()\n",
    "    return torch.tensor(AttentiveFP, dtype=torch.float)\n",
    "\n",
    "def get_edge_index(mol):\n",
    "    row, col = [], []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        row += [start, end]\n",
    "        col += [end, start]\n",
    "        \n",
    "    return torch.tensor([row, col], dtype=torch.long)\n",
    "\n",
    "from torch_geometric.data.dataloader import DataLoader\n",
    "\n",
    "def prepare_dataloader(mol_list, batch_size=3):\n",
    "    data_list = []\n",
    "\n",
    "    for i, mol in enumerate(mol_list):\n",
    "\n",
    "        x = get_atom_features(mol)\n",
    "        edge_index = get_edge_index(mol)\n",
    "\n",
    "        data = torch_geometric.data.Data(x=x, edge_index=edge_index)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return DataLoader(data_list, batch_size=batch_size, shuffle=False), data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Data(edge_index=[2, 46], x=[22, 36]), Data(edge_index=[2, 66], x=[32, 36]), Data(edge_index=[2, 38], x=[18, 36]), Data(edge_index=[2, 42], x=[20, 36]), Data(edge_index=[2, 40], x=[19, 36]), Data(edge_index=[2, 68], x=[31, 36])]\nBatch(batch=[72], edge_index=[2, 150], ptr=[4], x=[72, 36])\n"
     ]
    }
   ],
   "source": [
    "smiles_list = ['Cc1cc(c(C)n1c2ccc(F)cc2)S(=O)(=O)NCC(=O)N',\n",
    "'CN(CC(=O)N)S(=O)(=O)c1c(C)n(c(C)c1S(=O)(=O)N(C)CC(=O)N)c2ccc(F)cc2',\n",
    "'Fc1ccc(cc1)n2cc(COC(=O)CBr)nn2',\n",
    "'CCOC(=O)COCc1cn(nn1)c2ccc(F)cc2',\n",
    "'COC(=O)COCc1cn(nn1)c2ccc(F)cc2',\n",
    "'Fc1ccc(cc1)n2cc(COCC(=O)OCc3cn(nn3)c4ccc(F)cc4)nn2']\n",
    "\n",
    "mol_list = [Chem.MolFromSmiles(smi) for smi in smiles_list]\n",
    "\n",
    "dloader, dlist = prepare_dataloader(mol_list)\n",
    "print(dlist)\n",
    "#[Data(edge_index=[2, 46], x=[22, 2]),\n",
    "# Data(edge_index=[2, 66], x=[32, 2]),\n",
    "# Data(edge_index=[2, 38], x=[18, 2]),\n",
    "# Data(edge_index=[2, 42], x=[20, 2]),\n",
    "# Data(edge_index=[2, 40], x=[19, 2]),\n",
    "# Data(edge_index=[2, 68], x=[31, 2])]\n",
    "\n",
    "for batch in dloader:\n",
    "  break\n",
    "\n",
    "print(batch)\n",
    "#把前三个原子拼接在一起训练，batch的作用\n",
    "#Batch(batch=[72], edge_index=[2, 150], x=[72, 2])"
   ]
  },
  {
   "source": [
    "![alt text](../metadata/Snipaste_2021-05-24_15-15-53.png \"png\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Define the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![alt text](../metadata/model.png \"png\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch.nn as nn \n",
    "\n",
    "class NeuralLoop(MessagePassing):\n",
    "    def __init__(self, atom_features, fp_size):\n",
    "        super(NeuralLoop, self).__init__(aggr='add')\n",
    "        self.H = nn.Linear(atom_features, atom_features)\n",
    "        self.W = nn.Linear(atom_features, fp_size)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # x shape: [Number of atoms in molecule, Number of atom features]; [N, in_channels]\n",
    "        # edge_index shape: [2, E]; E is the number of edges\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "    \n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # We simply sum all the neighbouring nodes (including self-loops)\n",
    "        # This is done implicitly by PyTorch-Geometric :)\n",
    "        return x_j \n",
    "    \n",
    "    def update(self, v):\n",
    "        \n",
    "        updated_atom_features = self.H(v).sigmoid()\n",
    "        updated_fingerprint = self.W(updated_atom_features).softmax(dim=-1)\n",
    "        \n",
    "        return updated_atom_features, updated_fingerprint # shape [N, atom_features]\n",
    "    \n",
    "class NeuralFP(nn.Module):\n",
    "    def __init__(self, atom_features=52, fp_size=2048):\n",
    "        super(NeuralFP, self).__init__()\n",
    "        \n",
    "        self.atom_features = atom_features\n",
    "        self.fp_size = fp_size\n",
    "        \n",
    "        self.loop1 = NeuralLoop(atom_features=atom_features, fp_size=fp_size)\n",
    "        self.loop2 = NeuralLoop(atom_features=atom_features, fp_size=fp_size)\n",
    "        self.loops = nn.ModuleList([self.loop1, self.loop2])\n",
    "        \n",
    "    def forward(self, data):\n",
    "        fingerprint = torch.zeros((data.batch.shape[0], self.fp_size), dtype=torch.float).cuda()\n",
    "        out = data.x\n",
    "        for idx, loop in enumerate(self.loops):\n",
    "            updated_atom_features, updated_fingerprint = loop(out, data.edge_index)\n",
    "            out = updated_atom_features\n",
    "            fingerprint += updated_fingerprint\n",
    "            \n",
    "        return scatter_add(fingerprint, data.batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 2048])\n"
     ]
    }
   ],
   "source": [
    "neural_fp = NeuralFP(atom_features=36, fp_size=2048).cuda()\n",
    "fps = neural_fp(batch.cuda()) # remember, batch size was 3\n",
    "print(fps.shape)\n",
    "# torch.Size([3, 2048])"
   ]
  },
  {
   "source": [
    "# Work on a real dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "_, (train, valid, test), _ = dc.molnet.load_delaney(featurizer='Raw')\n",
    "\n",
    "bs = 32\n",
    "\n",
    "train_loader, _ = prepare_dataloader(list(train.X), batch_size=bs)\n",
    "valid_loader, _ = prepare_dataloader(valid.X, bs)\n",
    "test_loader, _ = prepare_dataloader(test.X, bs)\n",
    "\n",
    "train_labels_loader = torch.utils.data.DataLoader(train.y, batch_size=bs)\n",
    "valid_labels_loader = torch.utils.data.DataLoader(valid.y, batch_size=bs)\n",
    "test_labels_loader = torch.utils.data.DataLoader(test.y, batch_size=bs)"
   ]
  },
  {
   "source": [
    "## build a smal MLP on top of our neural fingerprint."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_Regressor(nn.Module):\n",
    "    def __init__(self, atom_features=2, fp_size=2048, hidden_size=100):\n",
    "        super(MLP_Regressor, self).__init__()\n",
    "        self.neural_fp = neural_fp.cuda()\n",
    "        self.lin1 = nn.Linear(fp_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        fp = self.neural_fp(batch)\n",
    "        hidden = F.relu(self.dropout(self.lin1(fp)))\n",
    "        out = self.lin2(hidden)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "## Define our utility functions for training and validation:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, labels, reg):\n",
    "    batch.cuda()\n",
    "    labels.cuda()\n",
    "    out = reg(batch).cuda()\n",
    "    loss = torch.sqrt(F.mse_loss(out, labels.to(torch.float).cuda(), reduction='mean').cuda())\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def valid_step(batch, labels, reg):\n",
    "    batch.cuda()\n",
    "    labels.cuda()\n",
    "    out = reg(batch).cuda()\n",
    "    loss = torch.sqrt(F.mse_loss(out, labels.to(torch.float).cuda(), reduction='mean').cuda())\n",
    "    return loss\n",
    "\n",
    "def train_fn(train_loader, train_labels_loader, reg, opt):\n",
    "    reg.train()\n",
    "    total_loss = 0\n",
    "    for idx, (batch, labels) in enumerate(zip(train_loader, train_labels_loader)):\n",
    "        loss = train_step(batch, labels, reg).cuda()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(reg.parameters(), 1)    \n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    return total_loss/len(train_loader)\n",
    "\n",
    "def valid_fn(valid_loader, valid_labels_loader, reg):\n",
    "    reg.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch, labels) in enumerate(zip(valid_loader, valid_labels_loader)):\n",
    "            loss = valid_step(batch, labels, reg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    total_loss /= len(valid_loader)\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:10, Train loss: 0.9496783737478585, Valid loss: 1.0468420386314392\n",
      "Epoch:20, Train loss: 0.9487133601616169, Valid loss: 1.0484953075647354\n",
      "Epoch:30, Train loss: 0.9480453832396145, Valid loss: 1.0499243587255478\n",
      "Epoch:40, Train loss: 0.9466879347275043, Valid loss: 1.0511573404073715\n",
      "Epoch:50, Train loss: 0.946238885665762, Valid loss: 1.0523102283477783\n",
      "Epoch:60, Train loss: 0.9450802001459845, Valid loss: 1.0534222424030304\n",
      "Epoch:70, Train loss: 0.9447108383836418, Valid loss: 1.0544604808092117\n",
      "Epoch:80, Train loss: 0.9439025562385033, Valid loss: 1.0552864521741867\n",
      "Epoch:90, Train loss: 0.9430742037707361, Valid loss: 1.0559005737304688\n",
      "Epoch:100, Train loss: 0.9427291216521427, Valid loss: 1.0563311874866486\n",
      "Epoch:110, Train loss: 0.9422031599899818, Valid loss: 1.0564383417367935\n",
      "Epoch:120, Train loss: 0.9422001633150824, Valid loss: 1.056469202041626\n",
      "Epoch:130, Train loss: 0.9422807796248074, Valid loss: 1.0564972460269928\n",
      "Epoch:140, Train loss: 0.9419742781540443, Valid loss: 1.0565239489078522\n",
      "Epoch:150, Train loss: 0.9419495586691231, Valid loss: 1.0565439611673355\n",
      "Epoch:160, Train loss: 0.9418609984989824, Valid loss: 1.0565670728683472\n",
      "Epoch:170, Train loss: 0.9422200753771025, Valid loss: 1.0565843284130096\n",
      "Epoch:180, Train loss: 0.9418770917530718, Valid loss: 1.0566027760505676\n",
      "Epoch:190, Train loss: 0.941822062278616, Valid loss: 1.0566189289093018\n",
      "Epoch:200, Train loss: 0.9417973099083736, Valid loss: 1.0566339194774628\n",
      "Epoch:210, Train loss: 0.9418232440948486, Valid loss: 1.056639015674591\n",
      "Epoch:220, Train loss: 0.9414443990279888, Valid loss: 1.056640088558197\n",
      "Epoch:230, Train loss: 0.9417824190238426, Valid loss: 1.0566429942846298\n",
      "Epoch:240, Train loss: 0.941788819329492, Valid loss: 1.0566450357437134\n",
      "Epoch:250, Train loss: 0.9417691641840441, Valid loss: 1.056646078824997\n",
      "Epoch:260, Train loss: 0.9417829862956343, Valid loss: 1.056648537516594\n",
      "Epoch:270, Train loss: 0.9418003209706011, Valid loss: 1.0566475540399551\n",
      "Epoch:280, Train loss: 0.9416192872770901, Valid loss: 1.0566492825746536\n",
      "Epoch:290, Train loss: 0.941670639761563, Valid loss: 1.0566508769989014\n",
      "Epoch:300, Train loss: 0.941692434508225, Valid loss: 1.056653082370758\n",
      "Epoch:310, Train loss: 0.9416536072204853, Valid loss: 1.056653305888176\n",
      "Epoch:320, Train loss: 0.9416023205066549, Valid loss: 1.056653693318367\n",
      "Epoch:330, Train loss: 0.9417720149303305, Valid loss: 1.05665422976017\n",
      "Epoch:340, Train loss: 0.9414565152135389, Valid loss: 1.056654453277588\n",
      "Epoch:350, Train loss: 0.9418617877466925, Valid loss: 1.0566550344228745\n",
      "Epoch:360, Train loss: 0.9416300222791475, Valid loss: 1.05665522813797\n",
      "Epoch:370, Train loss: 0.9413907137410394, Valid loss: 1.0566550493240356\n",
      "Epoch:380, Train loss: 0.9415782135108421, Valid loss: 1.0566551238298416\n",
      "Epoch:390, Train loss: 0.941775669311655, Valid loss: 1.0566554963588715\n",
      "Epoch:400, Train loss: 0.9417935568710853, Valid loss: 1.0566554963588715\n",
      "Epoch:410, Train loss: 0.9417592533703508, Valid loss: 1.0566557347774506\n",
      "Epoch:420, Train loss: 0.9417268161115975, Valid loss: 1.0566558539867401\n",
      "Epoch:430, Train loss: 0.9416859478786074, Valid loss: 1.0566558241844177\n",
      "Epoch:440, Train loss: 0.9414230174031751, Valid loss: 1.0566557794809341\n",
      "Epoch:450, Train loss: 0.9417536731424003, Valid loss: 1.0566558837890625\n",
      "Epoch:460, Train loss: 0.9415920006817785, Valid loss: 1.0566559582948685\n",
      "Epoch:470, Train loss: 0.9417800553913774, Valid loss: 1.056656002998352\n",
      "Epoch:480, Train loss: 0.9417650288548963, Valid loss: 1.0566560178995132\n",
      "Epoch:490, Train loss: 0.941746619240991, Valid loss: 1.0566561073064804\n",
      "Epoch:500, Train loss: 0.941548111109898, Valid loss: 1.0566561818122864\n",
      "Epoch:510, Train loss: 0.9418322217875513, Valid loss: 1.0566562712192535\n",
      "Epoch:520, Train loss: 0.941623500708876, Valid loss: 1.0566562712192535\n",
      "Epoch:530, Train loss: 0.9416502261983937, Valid loss: 1.056656301021576\n",
      "Epoch:540, Train loss: 0.9416202265640785, Valid loss: 1.0566563606262207\n",
      "Epoch:550, Train loss: 0.9418178776214863, Valid loss: 1.0566563457250595\n",
      "Epoch:560, Train loss: 0.9419677154771213, Valid loss: 1.056656375527382\n",
      "Epoch:570, Train loss: 0.9415871603735562, Valid loss: 1.056656375527382\n",
      "Epoch:580, Train loss: 0.9416763926374501, Valid loss: 1.0566564053297043\n",
      "Epoch:590, Train loss: 0.9417478483298729, Valid loss: 1.0566564053297043\n",
      "Epoch:600, Train loss: 0.9418526702913744, Valid loss: 1.0566564351320267\n",
      "Epoch:610, Train loss: 0.9417336357050928, Valid loss: 1.0566564202308655\n",
      "Epoch:620, Train loss: 0.9417887761675078, Valid loss: 1.0566564798355103\n",
      "Epoch:630, Train loss: 0.9416292371421024, Valid loss: 1.0566564798355103\n",
      "Epoch:640, Train loss: 0.9415846754764688, Valid loss: 1.056656539440155\n",
      "Epoch:650, Train loss: 0.9417641697258785, Valid loss: 1.0566565096378326\n",
      "Epoch:660, Train loss: 0.9415164261028685, Valid loss: 1.0566565096378326\n",
      "Epoch:670, Train loss: 0.9415629752751055, Valid loss: 1.0566565841436386\n",
      "Epoch:680, Train loss: 0.9415128189941933, Valid loss: 1.0566565692424774\n",
      "Epoch:690, Train loss: 0.9417087230189093, Valid loss: 1.0566566288471222\n",
      "Epoch:700, Train loss: 0.9419338209875698, Valid loss: 1.0566566586494446\n",
      "Epoch:710, Train loss: 0.9420489196119637, Valid loss: 1.0566566735506058\n",
      "Epoch:720, Train loss: 0.9415835676522091, Valid loss: 1.0566566735506058\n",
      "Epoch:730, Train loss: 0.9415665947157761, Valid loss: 1.0566566735506058\n",
      "Epoch:740, Train loss: 0.9414843999106308, Valid loss: 1.056656688451767\n",
      "Epoch:750, Train loss: 0.9414940538077519, Valid loss: 1.0566566735506058\n",
      "Epoch:760, Train loss: 0.9417713818878963, Valid loss: 1.0566567331552505\n",
      "Epoch:770, Train loss: 0.9415510420141548, Valid loss: 1.0566567182540894\n",
      "Epoch:780, Train loss: 0.9418949340951854, Valid loss: 1.0566567480564117\n",
      "Epoch:790, Train loss: 0.9416203293307074, Valid loss: 1.0566567480564117\n",
      "Epoch:800, Train loss: 0.9415806634672756, Valid loss: 1.0566567480564117\n",
      "Epoch:810, Train loss: 0.9415619167788275, Valid loss: 1.0566567778587341\n",
      "Epoch:820, Train loss: 0.9413354211840136, Valid loss: 1.0566567927598953\n",
      "Epoch:830, Train loss: 0.9417577920288875, Valid loss: 1.0566567927598953\n",
      "Epoch:840, Train loss: 0.9419045941583042, Valid loss: 1.0566568821668625\n",
      "Epoch:850, Train loss: 0.9418622173112015, Valid loss: 1.0566568672657013\n",
      "Epoch:860, Train loss: 0.9418752481197489, Valid loss: 1.0566568821668625\n",
      "Epoch:870, Train loss: 0.9415873412428231, Valid loss: 1.0566568821668625\n",
      "Epoch:880, Train loss: 0.9419197316827446, Valid loss: 1.056656926870346\n",
      "Epoch:890, Train loss: 0.9416748943000004, Valid loss: 1.056656926870346\n",
      "Epoch:900, Train loss: 0.9415832634629875, Valid loss: 1.0566569864749908\n",
      "Epoch:910, Train loss: 0.941963304733408, Valid loss: 1.056657001376152\n",
      "Epoch:920, Train loss: 0.9416698977865022, Valid loss: 1.056657001376152\n",
      "Epoch:930, Train loss: 0.9417808384730898, Valid loss: 1.0566570162773132\n",
      "Epoch:940, Train loss: 0.941834007871562, Valid loss: 1.0566570609807968\n",
      "Epoch:950, Train loss: 0.9417132385845842, Valid loss: 1.0566570907831192\n",
      "Epoch:960, Train loss: 0.941583684806166, Valid loss: 1.0566571056842804\n",
      "Epoch:970, Train loss: 0.9415608007332374, Valid loss: 1.0566571354866028\n",
      "Epoch:980, Train loss: 0.9417703192809532, Valid loss: 1.056657150387764\n",
      "Epoch:990, Train loss: 0.9415623422326713, Valid loss: 1.0566571354866028\n",
      "Epoch:1000, Train loss: 0.9416461278652323, Valid loss: 1.0566571354866028\n"
     ]
    }
   ],
   "source": [
    "reg = MLP_Regressor(atom_features=36, fp_size=2048, hidden_size=100).cuda()\n",
    "optimizer = torch.optim.SGD(reg.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100)\n",
    "\n",
    "total_epochs = 1000\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    train_loss = train_fn(train_loader, train_labels_loader, reg, opt=optimizer)\n",
    "    valid_loss = valid_fn(valid_loader, valid_labels_loader, reg)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch:{epoch}, Train loss: {train_loss}, Valid loss: {valid_loss}')"
   ]
  },
  {
   "source": [
    "2 features：0.9302953"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}